from typing import Any, List

import torch.nn as nn
import torch
from transformers import BertModel, PreTrainedModel


class PILinkModel(nn.Module):
    def __init__(self, code_model: Any, nlp_model: BertModel):
        super(PILinkModel, self).__init__()
        self.nlp_model: BertModel = nlp_model
        # TODO: add code model

        total_hidden_size: int = nlp_model.config.hidden_size # + code_model.config.hidden_size
        linear_sizes: List[int] = [total_hidden_size, 512, 1]
        self.linears = nn.Sequential(
            nn.Linear(linear_sizes[0], linear_sizes[1]),
            nn.ReLU(),
            nn.Linear(linear_sizes[1], linear_sizes[2]),
            nn.Sigmoid()
        )
      
    def forward(self, nl_inputs):
        """
        Forward pass for the model.

        Args:
            nl_inputs (Dict): The natural language inputs to the model, generated by tokenizer. 
                                It is a dictionary with the following keys:
                                - input_ids (torch.Tensor): The input ids of the tokens.
                                - attention_mask (torch.Tensor): The attention mask of the tokens.
                                - token_type_ids (torch.Tensor): The token type ids of the tokens.
        Returns:
            torch.Tensor: The output tensor from the model.
        """
        nl_vec = self.issue_nlp_model(nl_inputs).last_hidden_state
        nl_vec = nl_vec[:,0,:] # torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)
        vec = nl_vec
        # vec = torch.cat((nl_vec, code_vec), dim=1)
        out = self.linears(vec)
        return out
    
    def load_linears_state_dict(self, state_dict):
        self.linears.load_state_dict(state_dict)
